________________________________________________________
			Statistics Concepts:
________________________________________________________

Mean:
	--> Represents the average value of a dataset.
	--> Calculated by summing up all values and dividing by the total number of values.

Harmonic Mean:
	--> A type of average that is used when dealing with rates, ratios, or other quantities that have a multiplicative relationship
	--> Calculated by taking the reciprocal of the arithmetic mean of the reciprocals of a set of values
	--> Gives more weight to the smaller values in the dataset, making it useful in situations where there are extreme values or outliers that can significantly influence the average

Median:
	--> Represents the middle value in a dataset.
	--> Less affected by outliers compared to the mean.

Mode:
	--> Represents the most frequent value(s) in a dataset.
	--> Can be applied to both numerical and categorical data.

Standard Deviation:
	--> Measures the spread or variability of a dataset.
	--> Indicates the average distance between data points and the mean.


________________________________________________________
			PANDAS:
________________________________________________________

Imports (typical setup):
	- import numpy as np
	- import pandas as pd

General Terminology Translation:
	- PANDAS Term = EXCEL Term 
	- DataFrame = Worksheet
		--> While an excel workbook can contain many worksheets, pandas DataFrame's exist independently
	- Series = Column
		--> Data structure which represents one column of a dataframe
	- Index = Row Headings
		--> Essentially, labels on the rows of the data
		--> If no index is specified, a RangeIndex is used by default (i.e. row 1=0, row2= 1)
		--> In excel, as opposed to A1:Z1, you could do populations.loc['Chicago'] where Chicago is the index (row name)
	- Row = Row
	- NaN = Empty Cell


Copies vs. in place operations:
	- Since most pandasa operations return copies of the DataFrame, you will either need to assign the result to a new variable or overwrite the old one

Constructing a DataFrame:
	- In excel, you may click into a cell and type in values
	- In pandas, you should pass in the values in the form of a python dictionary, where the keys are the column names and the values are each row
	In [3]: df = pd.DataFrame({"x": [1, 3, 5], "y": [2, 4, 6]})

	In [4]: df
	Out[4]: 
	   x  y
	0  1  2
	1  3  4
	2  5  6

Reading External Data: 
	In [5]: url = (
	   ...:     "https://raw.githubusercontent.com/pandas-dev"
	   ...:     "/pandas/main/pandas/tests/io/data/csv/tips.csv"
	   ...: )
	   ...: 

	In [6]: tips = pd.read_csv(url)
	
	In [7]: tips
	Out[7]: 
	     total_bill   tip     sex smoker   day    time  size
	0         16.99  1.01  Female     No   Sun  Dinner     2
	1         10.34  1.66    Male     No   Sun  Dinner     3
	2         21.01  3.50    Male     No   Sun  Dinner     3
	3         23.68  3.31    Male     No   Sun  Dinner     2
	4         24.59  3.61  Female     No   Sun  Dinner     4
	..          ...   ...     ...    ...   ...     ...   ...
	239       29.03  5.92    Male     No   Sat  Dinner     3
	240       27.18  2.00  Female    Yes   Sat  Dinner     2
	241       22.67  2.00    Male    Yes   Sat  Dinner     2
	242       17.82  1.75    Male     No   Sat  Dinner     2
	243       18.78  3.00  Female     No  Thur  Dinner     2

	[244 rows x 7 columns]
	
Limiting Output:
	tips.head(5) --> Get the first 5 rows
	tips.tail(5) --> Get the last 5 rows

Exporting Data: 
	- You can export a dataframe to a particualr file type (i.e. excel, csv)

Operations On Columns: 
	- Just look up as needed, but all the same functionality as excel
	- Pandas provides vectorized operations 
________________________________________________________
			Machine Learning Fundamentals:
________________________________________________________



Dataset = A dataset refers to a structured collection of data instances or samples that are used for training, testing, or evaluating a machine learning model

Data instance = An individual observation/example that comprises a dataset. For example, in a dataset for predicting housing prices, each data instance in this case could represent a single house and would consist of various attributes or features such as footage, numbers, location etc.

Attribute/Feature/Variable = A measurable property or characteristic of an object that is used as input to a machine learning model


Machine Learning Tasks = The specific types of problems/objectives that can be addressed using machine learning algorithms and techniques: 

	--> Classification: Involves assigning data instances to predefined (discrete) classes or categories based on their features/attributes (example: spam detection, image classifcation, sentiment analysis, disease diagnosis) 
		--> Binary Classification (2) = Positive/negative, Cat/dog, Spam/not spam
		--> Multiclass Classification  (more than 2) = Cat/dog/lizard/dolphin, organge/apple/pear
	--> Regression: Involves predicting a continous numeral value or a set of values. The goal is to build a model that can estimate or approximate the relationship between input features and the continous target variable. (example: predicting house prices, stock market prices)
	--> Clustering: Clustering is an unsupervised learning task where the goal is to group similar data instances together based on their inherent similarities. The model identifies patterns or structures in the data without using predefined class labels. (example: customer sementation, document clustering, imaeg segmentation)
	--> Anomoly Detection: Aims to identify rare or unusual data instances that deviate significantly from the expected patterns. The model learns the normal behavior from the majority of the data and flags instances that are considered abnormal or anomalous (example: fraud detection, network intrusion detection, equiment failure prediction)
	--> Recommendation Systems: Predict and suggest it.ems or content that a user might be interested in based on their preferences and behavior
	--> Natural Language Processing (NLP): NLP tasks involve processing and understanding human language. These tasks include text classification, sentiment analysis, named entity recognition, machine translation, text summarization, and question-answering systems.
	--> Reinforcement Learning: Reinforcement learning involves training an agent to make sequential decisions in an environment to maximize rewards. The agent learns through trial and error and receives feedback from the environment. This task is often used in robotics, game playing, and autonomous systems.


Types of Machine Learning: 
	- Supervised Learning = Uses labeled inputs to train models and learn outputs (i.e. You give a program different photos of animals, with labels indicating the type of animal)
	- Unsupervised Learning = Uses unlabeled data to learn about patterns in the data (i.e. You give a program different photos of animals, and it groups them together based on similarities - data is not labelled)
	- Reinforcement Learning = Agent learning in interactive environment based on rewards and penalties (i.e. Like training a dog, give rewards when does something correct, otherwise penalize)



Machine Learning: 
______
|     |		|------|
|Input| ==>  	|Model | ==> Output (Prediction 
|_____|		|______|

All of the inputs are known as the feature vector.


Types of features: (in machine learning, it is generally beneficial to represent all relevant data as numerical features BEFORE training a model)
	- Qualitative: 
		--> Represent characteristics or attributes that are non-numeric and typically fall into distinct categories or classes
		--> Categorical/nominal data (finite number of categories or groups) => example is gender, nation.
		--> Nominal data is a type of data that represents categories/labels without any inherent order or numerical value. Used alongside technique = One-hot encoding
		--> Ordinal data is a type of data that can be ordered (example: ranking satisfaction levels from 1 to 5 or age groups from baby to senior)
	- Quantitative: 
		--> Represent numerical measurements or quantities that are associated with objects or observations
		--> Can be further classified into continuous and discrete variables
			--> Continuous : Can take on any values
				- i.e. Height at any given time since height is something that can be any real number. Think : There is infinity values between 0 and 1. 
			--> Discrete : Can only take specific values
				- i.e. Number of easter eggs that you have collected 
One-hot encoding = A technique used to represent categorical variables as binary vectors. It is a common preprocessing step in machine learning to convert categorical data into a numerical format that can be effectively be used by machine learning algorithms. Essentially, each category (i.e. canada, usa, china) becomes a feature (column) and a data instance will indicate with a 1/0 as to whether or not a instance fits into each category.

Matrix: 
	--> Features matrix (say, X)
	--> Outcome vector (say, Y)

How does it work?:
	--> Each row (data instance) is passed into the model, and a prediction is made accordingly. Then, the resulting value is compared against the actual result. Adjusting the results and tweaking the model is what is called "training"


The categories of datasets:

--> Training Dataset:
	- The training dataset is the portion of the data used to train the machine learning model.
	- It contains a set of input data points (features) and their corresponding known output or target values (labels).
	- The model is trained on this dataset by iteratively adjusting its parameters to minimize the loss function, which measures the discrepancy between the predicted outputs and the true labels.
	- The training dataset is used to update the model's parameters, allowing it to learn patterns and relationships in the data.

--> Validation Dataset:

	- The validation dataset is a separate portion of the data that is not used during the training phase.
	- After training the model on the training dataset, it is evaluated on the validation dataset.
	- The validation dataset helps assess the model's generalization ability and its performance on unseen data.
	- By evaluating the model on the validation dataset, you can fine-tune hyperparameters, choose between different models, or make decisions on the best-performing model.

--> Testing Dataset:

	- The testing dataset is another independent portion of the data that is not used during training or validation.
	- Once the model is trained and fine-tuned using the training and validation datasets, it is evaluated on the testing dataset.
	- The testing dataset provides an unbiased assessment of the model's performance and its ability to generalize to new, unseen data.
	- This evaluation helps estimate the model's performance in real-world scenarios and provides insights into its effectiveness before deploying it in production.


Feedback Loop in the Training Phase:
	- During the training phase, the model is presented with the training dataset, and it makes predictions on the input data.
	- The predicted outcomes are compared to the true labels or target values in the training dataset.
	- The difference between the predicted values and the true values is quantified using a loss function.
	- The loss function measures how well the model is performing on the training data and serves as a guide for updating the model's parameters.
	- Through an optimization algorithm like gradient descent, the model iteratively adjusts its parameters to minimize the loss and improve its predictive performance.
	- This feedback loop continues for multiple iterations or epochs, gradually improving the model's ability to make accurate predictions.


Loss Functions: 
	--> Different types of machine learning tasks and models require different loss functions
	- L1 Loss (Mean Absolute Error):
		--> Measures the average absolute difference between predicted values and true values.
		--> Used in regression tasks to quantify the error between predictions and true values.
		--> Less commonly used in binary classification tasks.
		--> Minimizing L1 loss aims to make predictions as close as possible to the true values in an absolute sense.
	- Understanding the graph:
		--> The x axis represents the number of training iterations or epochs.
		--> The y axis represents the value of the L1 Loss
			--> Essentially, the graphs differ in the way that the loss (y axis values) are calculated
	- L2 Loss (Mean Squared Difference): 
		--> Calculates the average squared difference between predicted values and true values.
		--> Widely used in regression tasks to measure the error between predictions and true values.
		--> Can be adapted for binary classification, but other loss functions are typically preferred.
		--> Penalizes outliers and large errors more than L1 loss due to the squaring operation.
		--> Assumes a Gaussian distribution (bell curve) of errors, which may not be ideal for binary classification.
	- Understanding the graph:
		--> Similar to above
	- Binory Cross Entropy Loss: loss = -[y * log(p) + (1 - y) * log(1 - p)]
		--> You just need to know that loss decreases as the performance gets better

The Actual Training: 
In machine learning, a training epoch or iteration refers to a complete pass through the entire training dataset during the training process of a machine learning model.

Here's how it works:

	Training Dataset:
		--> The training dataset consists of a collection of input data points and their corresponding target values. For example, in a supervised learning problem, the training dataset contains pairs of input features and their associated labels.

	Batch Processing:
		--> During training, the dataset is typically divided into smaller subsets called batches. Each batch contains a fixed number of data points. The model updates its parameters based on the loss calculated for the predictions made on the data points within the batch.

	Iteration:
		--> An iteration refers to the process of making predictions on one batch of data and updating the model's parameters based on the loss calculated for that batch. After processing a batch, the model adjusts its parameters using an optimization algorithm, such as gradient descent, to minimize the loss and improve its predictions.

	Multiple Iterations per Epoch:
		--> To complete an epoch, the model goes through multiple iterations, processing all the batches in the training dataset. The number of iterations per epoch depends on the batch size and the total number of data points in the training dataset. If the batch size is equal to the dataset size, then there will be one iteration per epoch.

Essentially: 
		--> The training dataset is divided into smaller subsets called batches.
		--> Each batch is processed iteratively through the model.
		--> For each batch, the model makes predictions, calculates the loss between the predictions and the actual values, and updates its parameters using an optimization algorithm.
		--> This process of processing one batch and updating the model's parameters is referred to as an iteration.
		--> Multiple iterations are performed until all batches in the training dataset have been processed.
		--> When all batches have completed one iteration, an epoch is considered complete.
		--> The process of going through all the batches and completing multiple iterations is repeated for a specified number of epochs.

In summary, during an epoch, the training dataset is divided into batches, and the model iteratively processes each batch, calculates loss, and updates its parameters. This process is repeated for a specified number of epochs, allowing the model to gradually learn from the entire training dataset.


Metrics of Performance: 
		--> Accuracy (Overall correctness of the model's predictions - ratio of correct to total predictions)
		--> Precision (Ability to correctly identify positive instances out of all the instances it predicted as psotiive - True Positives / (True Positives + False Positives))
		--> Recall (Ability to identify positive instances out of all the positive instances - True Positives / (True Positives + False Negatives))
		--> F1 Score (Combines precision and recall into a single metric, a balanced measure of a model's performance - calculated as the harmonic mean of precision and recall)
			--> F1 Score = 2 * (Precision * Recall) / (Precision + Recall)
			--> F1 Score = 2 * (0.85 * 0.75) / (0.85 + 0.75) = 0.796

________________________________________________________
			Steps in Creating a Model:
________________________________________________________

Example - Create a model that can analyze credit card fraud: 
	--> Understand the Problem: Familiarize yourself with the problem of credit card fraud and its impact on financial institutions and customers. Gain insights into the types of fraud, common patterns, and challenges faced by the industry.

	--> Data Collection: Gather a dataset that consists of credit card transactions, both fraudulent and non-fraudulent. Ensure the dataset is representative and adequately covers different scenarios. The dataset should include various features such as transaction amount, location, time, and additional relevant information.

	--> Exploratory Data Analysis (EDA): Perform EDA on the dataset to gain a deeper understanding of the data. Analyze the distribution of fraudulent and non-fraudulent transactions, identify any missing or inconsistent values, and explore relationships between features. This step will help you uncover patterns and potential insights.

	--> Data Preprocessing: Prepare the data for model training. This includes handling missing values, encoding categorical variables, scaling numerical features, and splitting the data into training and testing sets. It's important to maintain the integrity of the data and ensure it's suitable for training a model.

	--> Model Selection and Training: Select an appropriate machine learning model for credit card fraud detection. Commonly used models include logistic regression, decision trees, random forests, and neural networks. Train the selected model on the training data and tune its hyperparameters to optimize performance.

	--> Model Evaluation: Evaluate the trained model using suitable performance metrics such as accuracy, precision, recall, F1 score, and ROC curve. Assess how well the model performs in detecting fraud and avoiding false positives and false negatives.

	--> Iterative Improvement: Based on the evaluation results, analyze the shortcomings of the model and iteratively refine it. You can experiment with different models, feature engineering techniques, and data preprocessing methods to enhance the model's performance.

	--> Deployment and Integration: Once you have a well-performing model, consider how it can be deployed and integrated into a real-world system. This may involve building an application or an API that can accept new transactions and provide predictions in real-time.

